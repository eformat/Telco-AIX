# Seed-OSS-36B-Instruct vLLM Deployment on OpenShift
# Usage: oc apply -f seed-oss-36b-deployment.yaml -n tme-aix
# Optimized configuration for maximum performance and GPU utilization

---
# ConfigMap for optimized model configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: seed-oss-36b-config
  namespace: tme-aix
  labels:
    app: seed-oss-36b
    variant: optimized
data:
  model_config.json: |
    {
      "model_name": "ByteDance-Seed/Seed-OSS-36B-Instruct",
      "max_model_len": 32768,
      "dtype": "auto",
      "trust_remote_code": true,
      "gpu_memory_utilization": 0.95,
      "tensor_parallel_size": 1,
      "download_dir": "/tmp/.cache",
      "load_format": "auto",
      "enforce_eager": false,
      "quantization": null,
      "seed": 42,
      "max_num_seqs": 1024,
      "max_num_batched_tokens": 32768,
      "enable_chunked_prefill": true,
      "enable_prefix_caching": true
    }
  startup_config.sh: |
    #!/bin/bash
    echo "Starting Optimized Seed-OSS-36B-Instruct vLLM server..."
    echo "GPU Information:"
    nvidia-smi
    echo "Optimized configuration for RTX PRO 6000 96GB"
    echo "Expected throughput: 25-35 tokens/s with batching"

---
# Service to expose the vLLM deployment
apiVersion: v1
kind: Service
metadata:
  name: seed-oss-36b-service
  namespace: tme-aix
  labels:
    app: seed-oss-36b
    variant: optimized
    component: vllm-server
spec:
  type: ClusterIP
  selector:
    app: seed-oss-36b
    variant: optimized
    component: vllm-server
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http

---
# Route for external HTTPS access
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: seed-oss-36b-route
  namespace: tme-aix
  labels:
    app: seed-oss-36b
    variant: optimized
    component: vllm-server
  annotations:
    haproxy.router.openshift.io/timeout: "900s"
spec:
  host: seed-oss-36b-tme-aix.apps.sandbox02.narlabs.io
  to:
    kind: Service
    name: seed-oss-36b-service
    weight: 100
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None

---
# Optimized Deployment for vLLM server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: seed-oss-36b-vllm
  namespace: tme-aix
  labels:
    app: seed-oss-36b
    variant: optimized
    component: vllm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: seed-oss-36b
      variant: optimized
      component: vllm-server
  template:
    metadata:
      labels:
        app: seed-oss-36b
        variant: optimized
        component: vllm-server
    spec:
      initContainers:
      - name: setup-permissions
        image: vllm/vllm-openai:v0.10.1.1
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "Setting up optimized configuration..."
            mkdir -p /tmp/.cache /tmp/.triton /tmp/transformers /tmp/.flashinfer
            chmod -R 777 /tmp/.cache /tmp/.triton /tmp/transformers /tmp/.flashinfer
            pip install git+https://github.com/huggingface/transformers.git --target /tmp/transformers --no-cache-dir
            echo "Optimized setup complete"
        volumeMounts:
        - name: cache
          mountPath: /tmp/.cache
        - name: transformers-lib
          mountPath: /tmp/transformers
      containers:
      - name: seed-server
        image: vllm/vllm-openai:v0.10.1.1
        args:
          - "--model"
          - "ByteDance-Seed/Seed-OSS-36B-Instruct"
          - "--trust-remote-code"
          - "--max-model-len"
          - "32768"
          - "--max-num-seqs"
          - "1024"
          - "--max-num-batched-tokens"
          - "32768"
          - "--enable-chunked-prefill"
          - "--enable-prefix-caching"
          - "--gpu-memory-utilization"
          - "0.95"
          - "--swap-space"
          - "8"
          - "--disable-log-stats"
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: seed-oss-hf-token
              key: token
              optional: true
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: seed-oss-hf-token
              key: token
              optional: true
        - name: HF_HOME
          value: "/tmp/.cache"
        - name: TRANSFORMERS_CACHE
          value: "/tmp/.cache"
        - name: HF_HUB_CACHE
          value: "/tmp/.cache"
        - name: PYTHONPATH
          value: "/tmp/transformers:$PYTHONPATH"
        - name: TRITON_CACHE_DIR
          value: "/tmp/.triton"
        - name: FLASHINFER_WORKSPACE_DIR
          value: "/tmp/.flashinfer"
        - name: XDG_CACHE_HOME
          value: "/tmp/.cache"
        - name: VLLM_USE_V1
          value: "0"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NCCL_DEBUG
          value: "WARN"
        ports:
        - containerPort: 8000
          protocol: TCP
          name: http
        resources:
          requests:
            memory: "90Gi"
            cpu: "20"
            nvidia.com/gpu: "1"
          limits:
            memory: "96Gi"
            cpu: "32"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: cache
          mountPath: /tmp/.cache
        - name: transformers-lib
          mountPath: /tmp/transformers
        - name: shm
          mountPath: /dev/shm
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 900
          periodSeconds: 60
          timeoutSeconds: 30
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 5
      volumes:
      - name: cache
        emptyDir:
          sizeLimit: 120Gi
      - name: transformers-lib
        emptyDir:
          sizeLimit: 8Gi
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
# GPU scheduling handled by nvidia.com/gpu resource requests
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule